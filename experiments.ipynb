{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,818,048 || all params: 1,238,632,448 || trainable%: 0.2275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# GPU가 있으면 사용\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# LoRA 설정: r, alpha, dropout 등은 상황에 맞게 조절합니다.\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"mlp.down_proj\",\n",
    "                    \"mlp.gate_proj\",\n",
    "                    \"mlp.up_proj\",\n",
    "                    \"self_attn.k_proj\",\n",
    "                    \"self_attn.o_proj\",\n",
    "                    \"self_attn.q_proj\",\n",
    "                    \"self_attn.v_proj\"],  # 모델에 따라 target 모듈 이름이 다를 수 있습니다.\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# base_model에 LoRA 어댑터 적용 (원래 파라미터는 동결되고, LoRA 파라미터만 업데이트됨)\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)\n",
    "\n",
    "# 파인튜닝 동안 KL 계산을 위한 기준 모델(reference model)은 사전학습된 원 모델을 그대로 사용합니다.\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "ref_model.eval()  # 기준 모델은 업데이트하지 않습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################\n",
    "# 2. 더미 RL 데이터셋 정의\n",
    "#############################\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    간단한 더미 데이터셋: \n",
    "      - 고정된 프롬프트(\"Hello, how are you?\")를 토크나이즈하여 입력으로 사용합니다.\n",
    "      - 행동(actions)은 입력 토큰과 동일하게 설정 (실제 상황에서는 모델이 생성한 행동)\n",
    "      - old_log_probs와 advantages는 더미 값입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, num_samples=100, seq_len=16):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.prompt = \"Hello, how are you?\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.seq_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)         # [seq_len]\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)   # [seq_len]\n",
    "        # 더미 행동: 여기서는 단순히 input_ids를 행동으로 사용\n",
    "        actions = input_ids.clone()\n",
    "        # 더미 old_log_probs 및 advantages (각 토큰에 대해)\n",
    "        old_log_probs = torch.randn(self.seq_len)\n",
    "        advantages = torch.randn(self.seq_len)\n",
    "        # 더미 reward (추후 확장 시 활용)\n",
    "        rewards = torch.randn(self.seq_len)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"actions\": actions,\n",
    "            \"old_log_probs\": old_log_probs,\n",
    "            \"advantages\": advantages,\n",
    "            \"rewards\": rewards,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GRPOTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(completion)) \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     17\u001b[0m training_args \u001b[38;5;241m=\u001b[39m GRPOConfig(output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2-0.5B-GRPO\u001b[39m\u001b[38;5;124m\"\u001b[39m, logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mGRPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: GRPOTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    return [-abs(20 - len(completion)) for completion in completions]\n",
    "training_args = GRPOConfig(output_dir=\"Qwen2-0.5B-GRPO\", logging_steps=10)\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_len,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습 시작\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
